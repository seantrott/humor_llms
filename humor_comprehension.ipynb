{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd1d194f",
   "metadata": {},
   "source": [
    "# Does GPT-3 \"get\" the joke?\n",
    "\n",
    "*Sean Trott*\n",
    "\n",
    "**Goal**: Determine whether GPT-3 is sensitive to humor manipulations, either in the form of `surprisal` or correctly responding to comprehension questions about the meaning of a joke.\n",
    "\n",
    "Outline of code:\n",
    "\n",
    "- Setup:  \n",
    "- Part 1: Surprisals.\n",
    "- Part 2: Comprehension. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a3a66f",
   "metadata": {},
   "source": [
    "## Setup \n",
    "\n",
    "### Set up API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b30cc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14e606d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c4ff30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'  # makes figs nicer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f91fcfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in key\n",
    "with open('gpt_key', 'r') as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "org = lines[0]\n",
    "api_key = lines[1]\n",
    "openai.organization = org # org\n",
    "openai.api_key = api_key # api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7ea168",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7363f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/raw/coulson_stimuli.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eab8dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Exp Code</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Correct</th>\n",
       "      <th>First Five Words</th>\n",
       "      <th>Group Number</th>\n",
       "      <th>Same/Different</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>J</td>\n",
       "      <td>1017</td>\n",
       "      <td>A committee keeps minutes and takes hours.</td>\n",
       "      <td>Committees are very efficient.</td>\n",
       "      <td>no</td>\n",
       "      <td>a committee keeps minutes and</td>\n",
       "      <td>1</td>\n",
       "      <td>Different</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S</td>\n",
       "      <td>3017</td>\n",
       "      <td>A committee keeps minutes and takes votes.</td>\n",
       "      <td>Committees keep records and make decisions.</td>\n",
       "      <td>yes</td>\n",
       "      <td>a committee keeps minutes and</td>\n",
       "      <td>1</td>\n",
       "      <td>Different</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S</td>\n",
       "      <td>4013</td>\n",
       "      <td>A device for finding furniture in the dark is ...</td>\n",
       "      <td>The candle gives off light.</td>\n",
       "      <td>yes</td>\n",
       "      <td>a device for finding furniture</td>\n",
       "      <td>2</td>\n",
       "      <td>Same</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>J</td>\n",
       "      <td>2013</td>\n",
       "      <td>A device for finding furniture in the dark is ...</td>\n",
       "      <td>People run into furniture when it's dark.</td>\n",
       "      <td>yes</td>\n",
       "      <td>a device for finding furniture</td>\n",
       "      <td>2</td>\n",
       "      <td>Same</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E</td>\n",
       "      <td>6056</td>\n",
       "      <td>A good source of vitamin A is orange vegetable...</td>\n",
       "      <td>Carrots contain vitamin A.</td>\n",
       "      <td>yes</td>\n",
       "      <td>a good source of vitamin</td>\n",
       "      <td>3</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Condition  Exp Code                                           Sentence  \\\n",
       "0         J      1017         A committee keeps minutes and takes hours.   \n",
       "1         S      3017         A committee keeps minutes and takes votes.   \n",
       "2         S      4013  A device for finding furniture in the dark is ...   \n",
       "3         J      2013  A device for finding furniture in the dark is ...   \n",
       "4         E      6056  A good source of vitamin A is orange vegetable...   \n",
       "\n",
       "                                        Answer Correct  \\\n",
       "0               Committees are very efficient.      no   \n",
       "1  Committees keep records and make decisions.     yes   \n",
       "2                  The candle gives off light.     yes   \n",
       "3    People run into furniture when it's dark.     yes   \n",
       "4                   Carrots contain vitamin A.     yes   \n",
       "\n",
       "                 First Five Words  Group Number Same/Different  \n",
       "0   a committee keeps minutes and             1      Different  \n",
       "1   a committee keeps minutes and             1      Different  \n",
       "2  a device for finding furniture             2           Same  \n",
       "3  a device for finding furniture             2           Same  \n",
       "4        a good source of vitamin             3              E  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec427aa1",
   "metadata": {},
   "source": [
    "## Part 1: Surprisals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f913d93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import backoff  # for exponential backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c037949",
   "metadata": {},
   "outputs": [],
   "source": [
    "@backoff.on_exception(backoff.expo, openai.error.RateLimitError)\n",
    "def get_response(prompt, model, tokens = 0):\n",
    "    response = openai.Completion.create(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        temperature=0,\n",
    "        max_tokens=tokens, ### 0\n",
    "        logprobs = 0,\n",
    "        top_p=1,\n",
    "        echo = True\n",
    "    )\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7934a896",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['ada', 'babbage', 'curie', 'davinci', \n",
    "          'text-ada-001', 'text-babbage-001', 'text-curie-001', 'text-davinci-002']\n",
    "# models = ['ada']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dd70eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    \n",
    "    sentence = row['Sentence']\n",
    "    \n",
    "    ## Establish final word\n",
    "    final_word = \" \" + sentence.replace(\".\", \"\").split()[-1]\n",
    "    \n",
    "    ## Establish num_tokens for final word\n",
    "    num_tokens = len(get_response(final_word, \"ada\", tokens = 1).to_dict()['choices'][0]['logprobs']['tokens']) - 1\n",
    "    \n",
    "    for model in models:    \n",
    "        \n",
    "        ## Get responses\n",
    "        response = get_response(sentence, model = model)\n",
    "        \n",
    "        ## Extract tokenized representation\n",
    "        tokens = response.to_dict()['choices'][0]['logprobs']['tokens']\n",
    "        identified_token = tokens[-2]\n",
    "        \n",
    "        ### Handle multi-token words\n",
    "        if num_tokens > 1:\n",
    "            identified_token = ''.join(tokens[-(num_tokens + 1):-1])\n",
    "            log_prob = sum(response.to_dict()['choices'][0]['logprobs']['token_logprobs'][-(num_tokens + 1):-1])\n",
    "            if identified_token != final_word:\n",
    "                print(tokens)\n",
    "                print(num_tokens)\n",
    "                print(final_word)\n",
    "                print(identified_token)\n",
    "        else:\n",
    "            log_prob = response.to_dict()['choices'][0]['logprobs']['token_logprobs'][-2]\n",
    "\n",
    "        results.append({\n",
    "            'sentence': sentence,\n",
    "            'model': model,\n",
    "            'condition': row['Condition'],\n",
    "            'log_prob': log_prob,\n",
    "            'num_tokens': num_tokens,\n",
    "            'identified_token': identified_token,\n",
    "            'surprisal': -log_prob,\n",
    "            'final_word': final_word\n",
    "        })\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8bebc311",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4a73638c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1172, 8)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "51450d0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_results['final_word'] == df_results['identified_token']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2802113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_csv(\"data/processed/results_gpt.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be02900e",
   "metadata": {},
   "source": [
    "## Part 2: Comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d2a89b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this task, you will be presented with a series of sentences. Some sentences will be serious statements, while others will be clever quips/jokes. In both cases your job is to determine whether the meaning of the second sentence is implied by the meaning of the first. If the two sentences have a similar meaning, answer \"yes\". If the two sentences do not have consistent meanings, answer \"no\".\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/raw/instructions0shot.txt\", \"r\") as f:\n",
    "    instructions0shot = f.read()\n",
    "print(instructions0shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9a53559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this task, you will be presented with a series of sentences. Some sentences will be serious statements, while others will be clever quips/jokes. In both cases your job is to determine whether the meaning of the second sentence is implied by the meaning of the first. If the two sentences have a similar meaning, answer \"yes\". If the two sentences do not have consistent meanings, answer \"no\".\n",
      "\n",
      "Sentence 1: The heater kept it warm inside.\n",
      "Sentence 2: It was cold in the house.\n",
      "\n",
      "Does Sentence 2 match Sentence 1?\n",
      "\n",
      "Answer: No.\n",
      "\n",
      "Sentence 1: Our child has a great deal of willpower -- and even more won't power.\n",
      "Sentence 2: Our child is very stubborn.\n",
      "\n",
      "Does Sentence 2 match Sentence 1?\n",
      "\n",
      "Answer: Yes.\n",
      "\n",
      "Sentence 1: The teacher spoke while the students listened.\n",
      "Sentence 2: The students joined in the conversation.\n",
      "\n",
      "Does Sentence 2 match Sentence 1?\n",
      "\n",
      "Answer: No.\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/raw/instructions1shot.txt\", \"r\") as f:\n",
    "    instructions1shot = f.read()\n",
    "print(instructions1shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0ca6ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(sentence, answer, prompt_shot, response):\n",
    "    prompt = \"{p}\\n\\nSentence 1: {x}\\n\\nSentence 2: {y}\".format(\n",
    "        p = prompt_shot, x = sentence, y = answer)\n",
    "    prompt += \"\\n\\nAnswer: {r}\".format(r = response)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1706b7",
   "metadata": {},
   "source": [
    "### 0-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d1d3b5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"text-davinci-002\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fed10af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 400/400 [04:56<00:00,  1.35it/s]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    \n",
    "    sentence = row['Sentence']\n",
    "    answer = row['Answer']\n",
    "    \n",
    "    yes_prompt = format_prompt(sentence, prompt_shot = instructions0shot, answer = answer, response = \"Yes\")\n",
    "    no_prompt = format_prompt(sentence, prompt_shot = instructions0shot, answer = answer, response = \"No\")\n",
    "    \n",
    "    for model in models:\n",
    "\n",
    "        ## Get responses\n",
    "        yes_response = get_response(yes_prompt, model = model)\n",
    "        no_response = get_response(no_prompt, model = model)\n",
    "        \n",
    "        ## Extract tokenized representations: YES\n",
    "        yes_tokens = yes_response.to_dict()['choices'][0]['logprobs']['tokens']\n",
    "        yes_identified_token = yes_tokens[-1]\n",
    "        if yes_identified_token != \" Yes\":\n",
    "            print(yes_identified_token) \n",
    "            \n",
    "        ## Extract tokenized representations: NO\n",
    "        no_tokens = no_response.to_dict()['choices'][0]['logprobs']['tokens']\n",
    "        no_identified_token = no_tokens[-1]\n",
    "        if no_identified_token != \" No\":\n",
    "            print(no_identified_token)\n",
    "        \n",
    "        \n",
    "        ## Get logprobs\n",
    "        lp_yes = yes_response.to_dict()['choices'][0]['logprobs']['token_logprobs'][-1]\n",
    "        lp_no = no_response.to_dict()['choices'][0]['logprobs']['token_logprobs'][-1]\n",
    "            \n",
    "        \n",
    "        results.append({\n",
    "            'model': model,\n",
    "            'sentence': sentence,\n",
    "            'answer': answer,\n",
    "            'shots': 0, ### adjust with different runs\n",
    "            'condition': row['Condition'],\n",
    "            'correct': row['Correct'],\n",
    "            'yes_lp': lp_yes, \n",
    "            'no_lp': lp_no\n",
    "        })\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "154c6cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5d9cef48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results['lp_ratio'] = df_results['yes_lp'] - df_results['no_lp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d4369719",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_csv(\"data/processed/comprehension_probe_0shot.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c171b936",
   "metadata": {},
   "source": [
    "### 1-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7925a15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"text-davinci-002\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a1a63b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 400/400 [05:08<00:00,  1.30it/s]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    \n",
    "    sentence = row['Sentence']\n",
    "    answer = row['Answer']\n",
    "    \n",
    "    yes_prompt = format_prompt(sentence, prompt_shot = instructions1shot, answer = answer, response = \"Yes\")\n",
    "    no_prompt = format_prompt(sentence, prompt_shot = instructions1shot, answer = answer, response = \"No\")\n",
    "    \n",
    "    \n",
    "    for model in models:\n",
    "\n",
    "        ## Get responses\n",
    "        yes_response = get_response(yes_prompt, model = model)\n",
    "        no_response = get_response(no_prompt, model = model)\n",
    "        \n",
    "        ## Extract tokenized representations: YES\n",
    "        yes_tokens = yes_response.to_dict()['choices'][0]['logprobs']['tokens']\n",
    "        yes_identified_token = yes_tokens[-1]\n",
    "        if yes_identified_token != \" Yes\":\n",
    "            print(yes_identified_token) \n",
    "            \n",
    "        ## Extract tokenized representations: NO\n",
    "        no_tokens = no_response.to_dict()['choices'][0]['logprobs']['tokens']\n",
    "        no_identified_token = no_tokens[-1]\n",
    "        if no_identified_token != \" No\":\n",
    "            print(no_identified_token)\n",
    "        \n",
    "        \n",
    "        ## Get logprobs\n",
    "        lp_yes = yes_response.to_dict()['choices'][0]['logprobs']['token_logprobs'][-1]\n",
    "        lp_no = no_response.to_dict()['choices'][0]['logprobs']['token_logprobs'][-1]\n",
    "            \n",
    "        \n",
    "        results.append({\n",
    "            'model': model,\n",
    "            'sentence': sentence,\n",
    "            'answer': answer,\n",
    "            'shots': 1, ### adjust with different runs\n",
    "            'condition': row['Condition'],\n",
    "            'correct': row['Correct'],\n",
    "            'yes_lp': lp_yes, \n",
    "            'no_lp': lp_no\n",
    "        })\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cbd0dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "230b5b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results['lp_ratio'] = df_results['yes_lp'] - df_results['no_lp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26e9bb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_csv(\"data/processed/comprehension_probe_1shot.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ef61f8",
   "metadata": {},
   "source": [
    "## Supplementary: all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7b2060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['ada', 'babbage', 'curie', 'davinci', \n",
    "          'text-ada-001', 'text-babbage-001', 'text-curie-001', 'text-davinci-002']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "299f9ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 400/400 [18:29<00:00,  2.77s/it]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    \n",
    "    sentence = row['Sentence']\n",
    "    answer = row['Answer']\n",
    "    \n",
    "    yes_prompt = format_prompt(sentence, prompt_shot = instructions0shot, answer = answer, response = \"Yes\")\n",
    "    no_prompt = format_prompt(sentence, prompt_shot = instructions0shot, answer = answer, response = \"No\")\n",
    "    \n",
    "    for model in models:\n",
    "\n",
    "        ## Get responses\n",
    "        yes_response = get_response(yes_prompt, model = model)\n",
    "        no_response = get_response(no_prompt, model = model)\n",
    "        \n",
    "        ## Extract tokenized representations: YES\n",
    "        yes_tokens = yes_response.to_dict()['choices'][0]['logprobs']['tokens']\n",
    "        yes_identified_token = yes_tokens[-1]\n",
    "        if yes_identified_token != \" Yes\":\n",
    "            print(yes_identified_token) \n",
    "            \n",
    "        ## Extract tokenized representations: NO\n",
    "        no_tokens = no_response.to_dict()['choices'][0]['logprobs']['tokens']\n",
    "        no_identified_token = no_tokens[-1]\n",
    "        if no_identified_token != \" No\":\n",
    "            print(no_identified_token)\n",
    "        \n",
    "        \n",
    "        ## Get logprobs\n",
    "        lp_yes = yes_response.to_dict()['choices'][0]['logprobs']['token_logprobs'][-1]\n",
    "        lp_no = no_response.to_dict()['choices'][0]['logprobs']['token_logprobs'][-1]\n",
    "            \n",
    "        \n",
    "        results.append({\n",
    "            'model': model,\n",
    "            'sentence': sentence,\n",
    "            'answer': answer,\n",
    "            'shots': 0, ### adjust with different runs\n",
    "            'condition': row['Condition'],\n",
    "            'correct': row['Correct'],\n",
    "            'yes_lp': lp_yes, \n",
    "            'no_lp': lp_no\n",
    "        })\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f7dd5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d541887",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results['lp_ratio'] = df_results['yes_lp'] - df_results['no_lp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8acfc23c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3200, 9)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "20f953da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_csv(\"data/processed/all_models_comprehension_scaling_analysis.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d2cebb",
   "metadata": {},
   "source": [
    "## GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20c509df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt_gpt4(sentence, answer, prompt_shot):\n",
    "    prompt = \"{p}\\n\\nSentence 1: {x}\\n\\nSentence 2: {y}\".format(\n",
    "        p = prompt_shot, x = sentence, y = answer)\n",
    "    prompt += \"\\n\\nAnswer: \"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1caf7f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@backoff.on_exception(backoff.expo, openai.error.RateLimitError)\n",
    "def pred_tokens(prompt, n=10, model=\"gpt-4\"):\n",
    "    \"\"\"Get response.\"\"\"\n",
    "    output = openai.ChatCompletion.create(\n",
    "        model = model,\n",
    "        temperature = 0,\n",
    "        messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant. Your job is to figure out which sentences follow logically from which other sentences.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "      max_tokens=3,\n",
    "      top_p=1\n",
    "        )\n",
    "\n",
    "    return output# output['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ef31f423",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 400/400 [05:16<00:00,  1.26it/s]\n"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    \n",
    "    sentence = row['Sentence']\n",
    "    answer = row['Answer']\n",
    "    \n",
    "    prompt = format_prompt_gpt4(sentence, prompt_shot = instructions0shot, answer = answer)\n",
    "    \n",
    "    response = pred_tokens(prompt)\n",
    "    extracted_response = response['choices'][0]['message']['content']\n",
    "\n",
    "    answers.append({'response': extracted_response,\n",
    "                    'word': sentence,\n",
    "                   'answer': answer})\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e2b10afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_answers = pd.DataFrame(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c36fea6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_answers['correct'] = df['Correct']\n",
    "df_answers['response_lower'] = df_answers['response'].str.lower()\n",
    "df_answers['condition'] = df['Condition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "49c89d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.885"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_answers['response'].str.lower() == df_answers['correct']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "42debb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_answers.to_csv(\"data/processed/gpt4_comprehension.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f73d03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
